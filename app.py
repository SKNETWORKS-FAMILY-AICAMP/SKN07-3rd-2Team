import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough
from langchain.callbacks import get_openai_callback
import os


# í…ìŠ¤íŠ¸ ìš”ì•½ í•¨ìˆ˜
def summarize_document(document):
    llm = ChatOpenAI(model_name="gpt-4-0613", temperature=0)
    summary_prompt = "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì–´ë–¤ ì œí’ˆì— ëŒ€í•œ ì„¤ëª…ì„œì¸ì§€ ê°„ëµíˆ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n" + document
    summary = llm.predict(summary_prompt)
    return summary


# ë°ì´í„° ì—…ë¡œë“œ ë° í¬ë¡œë§ˆDB ì €ì¥
def init(uploaded_file):
    if uploaded_file is not None:
        # íŒŒì¼ ì €ì¥ í›„ ë¡œë“œ
        file_path = f"./temp/{uploaded_file.name}"
        os.makedirs("./temp", exist_ok=True)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # ë°ì´í„° ë¡œë“œ (PDF íŒŒì¼)
        loader = PyPDFLoader(file_path)
        document = loader.load()
        
        # ë°ì´í„° ë¶„í• 
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
        texts = text_splitter.split_documents(document)
        db = getDB()
        
        # Chroma DBì— ì €ì¥
        db.add_documents(texts)
        st.success("PDF íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # í…ìŠ¤íŠ¸ ìš”ì•½
        chunk = '\n'.join([text.page_content for text in texts])
        summary = summarize_document(chunk[:3000])
        
        # ì‚¬ì´ë“œë°”ì— ìš”ì•½ í‘œì‹œ
        st.sidebar.subheader("ğŸ“œ PDF ìš”ì•½")
        st.sidebar.write(summary)


# í…ìŠ¤íŠ¸ ì„ë² ë”©
def getDB():
    # ì €ì¥ ë° ê²€ìƒ‰
    embeddings = OpenAIEmbeddings()
    docsearch = Chroma(
        persist_directory="./db",  # ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        embedding_function=embeddings
    )
    return docsearch


# RAG ì²´ì¸ ìƒì„±
def getRagChain():
    retriever = getDB().as_retriever()
    rag_prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI(model_name="gpt-4-0613", temperature=0)
    
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()} 
        | rag_prompt 
        | llm 
    )
    return rag_chain


# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
def generate_answer(question):
    rag_chain = getRagChain()
    answer = rag_chain.invoke(question).content
    return answer



# í˜ì´ì§€ ì„¸íŒ…
st.set_page_config(page_title="SKNETWORKS-FAMILY-AICAMP/SKN07-3rd-2Team", layout='wide')
st.title('ğŸ“± ìŠ¤ë§ˆíŠ¸í° ì‚¬ìš©ë©”ë‰´ì–¼ ê¸°ë°˜ Q&A')
# st.header('ì œí’ˆ: Samsung S25')

# íŒŒì¼ ì—…ë¡œë“œ
with st.sidebar:
    uploaded_file = st.file_uploader("ğŸ—‚ï¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])
    if uploaded_file:
        init(uploaded_file)

# ì´ˆê¸°í™”
if 'conversation' not in st.session_state:
    st.session_state.conversation = []
    
with st.expander("ì§ˆë¬¸&ë‹µë³€ íˆìŠ¤í† ë¦¬ ë³´ê¸°", expanded=False):
    for q, a in st.session_state.conversation:
        with st.chat_message('user'):
            st.write(q)
        with st.chat_message('assistant'):
            st.write(a)

# í”„ë¡¬í”„íŠ¸ ì…ë ¥ box
question = st.chat_input('ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”')
if question:
    with st.chat_message('user'):
        st.write(question)
    
    with st.spinner('ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...'):
        with get_openai_callback() as cost:
            answer = generate_answer(question)
            with st.chat_message('assistant'):
                st.write(answer)
            
            st.session_state.conversation.append((question, answer))
